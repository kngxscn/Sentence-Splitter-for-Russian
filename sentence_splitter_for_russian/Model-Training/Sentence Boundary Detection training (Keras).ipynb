{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce 840M (CNMeM is disabled, cuDNN Version is too old. Update to v5, was 2000.)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import re\n",
    "import string\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import random\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.constraints import maxnorm\n",
    "from keras.models import model_from_json\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten, Activation, Merge, Highway\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
    "import keras.optimizers\n",
    "from keras.regularizers import l2, l1\n",
    "from sklearn.cross_validation import train_test_split, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading pretrained character embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "header = ['Char']\n",
    "for i in range(64):\n",
    "    header.append('X' + str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings = pd.read_csv('char_embeddings_d150_tr1e6_w2_softmax_adagrad_spaces.csv', names=header)\n",
    "embeddings_dictionary = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(len(embeddings)):\n",
    "    vec = []\n",
    "    for j in xrange (64):\n",
    "        vec += [embeddings['X' + str(j+1)][i]]\n",
    "    embeddings_dictionary[unicode(embeddings['Char'][i], 'utf8')] = vec\n",
    "embeddings_dictionary[' '] = embeddings_dictionary['_']\n",
    "\n",
    "class Embeddings_Reader(dict):\n",
    "         def __missing__(self, key):\n",
    "            return embeddings_dictionary[u'UNK']\n",
    "        \n",
    "embeddings_lookup = Embeddings_Reader(embeddings_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading training data and extracting features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stops(char):\n",
    "    stop = \"\\?\\!\\.\"\n",
    "    m = re.search(r'^[{0}]$'.format(stop), char)\n",
    "    return m != None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithfin/anaconda2/envs/tensorflow_env/lib/python2.7/site-packages/ipykernel/__main__.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators; you can avoid this warning by specifying engine='python'.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "yandex_corpus = pd.read_csv('./1mcorpus/corpus.en_ru.1m.ru' , sep='##%##', names = ['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 : Length of the data set is 0\n",
      "Iteration 100000 : Length of the data set is 107473\n",
      "Iteration 200000 : Length of the data set is 215137\n",
      "Iteration 300000 : Length of the data set is 322575\n",
      "Iteration 400000 : Length of the data set is 433742\n",
      "Iteration 500000 : Length of the data set is 557080\n",
      "Iteration 600000 : Length of the data set is 674213\n",
      "Iteration 700000 : Length of the data set is 801662\n",
      "Iteration 800000 : Length of the data set is 914944\n",
      "Iteration 900000 : Length of the data set is 1029297\n"
     ]
    }
   ],
   "source": [
    "first_sentences = list(yandex_corpus['sentence'])\n",
    "stops_data = collections.deque([])\n",
    "pointer = 0\n",
    "radius = 7\n",
    "window_size = 2*radius+1\n",
    "sliding_window = collections.deque([], maxlen = window_size)\n",
    "dot_features = []\n",
    "\n",
    "for i in xrange(len(first_sentences)):\n",
    "    \n",
    "    initial_pointer = 0    \n",
    "    sentence = [' '] + list(unicode (first_sentences[i], 'utf8'))    \n",
    "    \n",
    "    if len(sliding_window) < window_size:\n",
    "        for charnum in range(len(sentence)):\n",
    "            if (charnum == len(sentence) - 1) & stops(sentence[charnum]):\n",
    "                sliding_window.append(sentence[charnum] + u'#')\n",
    "            else:\n",
    "                sliding_window.append(sentence[charnum])\n",
    "            pointer += 1\n",
    "            initial_pointer += 1\n",
    "            if pointer == window_size:\n",
    "                break\n",
    "    \n",
    "    if pointer < window_size:\n",
    "        continue\n",
    "    \n",
    "    for charnum in range (initial_pointer, len(sentence)):\n",
    "        if stops(sliding_window[radius][0]):                        \n",
    "            dot_features = list(sliding_window)[:radius] + list(sliding_window)[-radius:]\n",
    "            if (len (sliding_window[radius]) == 2):\n",
    "                label = 0\n",
    "            else:                \n",
    "                label = 1\n",
    "            vec_features = map (lambda x: embeddings_lookup[x[0]], dot_features)            \n",
    "            stops_data.append((label, vec_features))\n",
    "        if (charnum == len(sentence) - 1) & stops(sentence[charnum]):\n",
    "            sliding_window.append(sentence[charnum] + u'#')\n",
    "        else:\n",
    "            sliding_window.append(sentence[charnum])    \n",
    "    if i % 100000 == 0:        \n",
    "        print('Iteration %d : Length of the data set is %d' % (i, len(stops_data)))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239398\n"
     ]
    }
   ],
   "source": [
    "#Number of nonbreaking stop characters in the dataset\n",
    "counter = 0\n",
    "for i in range (len(stops_data)):\n",
    "    if stops_data[i][0] == 1:\n",
    "        counter +=1\n",
    "print (counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Validation run with random train/tast split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels, features = zip (*stops_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train, data_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array(data_train, dtype='float32')\n",
    "X_test = np.array(data_test, dtype='float32')\n",
    "\n",
    "y_train = np.array(labels_train)\n",
    "y_test = np.array(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape = X_train[0].shape))\n",
    "model.add(Dense(40))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "stop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=0, mode='auto')\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1028512 samples, validate on 114280 samples\n",
      "Epoch 1/200\n",
      "1028512/1028512 [==============================] - 50s - loss: 0.0607 - acc: 0.9763 - val_loss: 0.0470 - val_acc: 0.9806\n",
      "Epoch 2/200\n",
      "1028512/1028512 [==============================] - 45s - loss: 0.0449 - acc: 0.9817 - val_loss: 0.0456 - val_acc: 0.9817\n",
      "Epoch 3/200\n",
      "1028512/1028512 [==============================] - 56s - loss: 0.0413 - acc: 0.9830 - val_loss: 0.0434 - val_acc: 0.9824\n",
      "Epoch 4/200\n",
      "1028512/1028512 [==============================] - 44s - loss: 0.0390 - acc: 0.9838 - val_loss: 0.0430 - val_acc: 0.9823\n",
      "Epoch 5/200\n",
      "1028512/1028512 [==============================] - 44s - loss: 0.0373 - acc: 0.9844 - val_loss: 0.0433 - val_acc: 0.9822\n",
      "Epoch 6/200\n",
      "1028512/1028512 [==============================] - 59s - loss: 0.0358 - acc: 0.9852 - val_loss: 0.0435 - val_acc: 0.9824\n",
      "Epoch 7/200\n",
      "1028512/1028512 [==============================] - 59s - loss: 0.0346 - acc: 0.9855 - val_loss: 0.0446 - val_acc: 0.9820\n",
      "Epoch 8/200\n",
      "1028512/1028512 [==============================] - 45s - loss: 0.0335 - acc: 0.9859 - val_loss: 0.0454 - val_acc: 0.9824\n",
      "Epoch 9/200\n",
      "1028512/1028512 [==============================] - 63s - loss: 0.0326 - acc: 0.9863 - val_loss: 0.0454 - val_acc: 0.9820\n",
      "Epoch 10/200\n",
      "1028512/1028512 [==============================] - 65s - loss: 0.0316 - acc: 0.9868 - val_loss: 0.0455 - val_acc: 0.9824\n",
      "Epoch 11/200\n",
      "1028512/1028512 [==============================] - 64s - loss: 0.0309 - acc: 0.9871 - val_loss: 0.0482 - val_acc: 0.9821\n",
      "Epoch 12/200\n",
      "1028512/1028512 [==============================] - 67s - loss: 0.0302 - acc: 0.9873 - val_loss: 0.0490 - val_acc: 0.9816\n",
      "Epoch 13/200\n",
      "1028512/1028512 [==============================] - 67s - loss: 0.0294 - acc: 0.9875 - val_loss: 0.0493 - val_acc: 0.9816\n",
      "Epoch 14/200\n",
      "1028512/1028512 [==============================] - 66s - loss: 0.0289 - acc: 0.9879 - val_loss: 0.0500 - val_acc: 0.9818\n",
      "Epoch 15/200\n",
      "1028512/1028512 [==============================] - 64s - loss: 0.0283 - acc: 0.9882 - val_loss: 0.0519 - val_acc: 0.9819\n",
      "Epoch 16/200\n",
      "1028512/1028512 [==============================] - 66s - loss: 0.0277 - acc: 0.9883 - val_loss: 0.0529 - val_acc: 0.9815\n",
      "114280/114280 [==============================] - 4s     \n",
      "\n",
      "\n",
      "Validation score : 0.0528505300126\n",
      "Validation accuracy : 0.981484084664\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=200, callbacks= [stop], shuffle=True,\n",
    "          validation_data=(X_test, y_test))\n",
    "\n",
    "score, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print ('\\n')\n",
    "print('Validation score :', score)\n",
    "print('Validation accuracy :', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "early stop monitored value should be changed to the validation loss and the patience should be decreased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splitpoints = range (0,len(stops_data),len(stops_data)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batches = []\n",
    "stops_data = list (stops_data)\n",
    "random.shuffle(stops_data)\n",
    "i_prev = 0\n",
    "for i in splitpoints[1:]:\n",
    "    batches.append (stops_data[i_prev:i])\n",
    "    i_prev = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validation_training = []\n",
    "validation_test = []\n",
    "indices = range (len(batches))\n",
    "for i in indices:\n",
    "    test = batches[i]\n",
    "    validation_test.append(test)\n",
    "    \n",
    "    training = []\n",
    "    training_indices = list (indices)\n",
    "    training_indices.remove(i)\n",
    "    \n",
    "    for j in training_indices:        \n",
    "        training += batches[j]\n",
    "    \n",
    "    validation_training.append(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step: 0 ...\n",
      "Done.\n",
      "Step 0 accuracy: 0.981728937187\n",
      "-----------------------------------\n",
      "\n",
      "Training step: 1 ...\n",
      "Done.\n",
      "Step 1 accuracy: 0.981466421936\n",
      "-----------------------------------\n",
      "\n",
      "Training step: 2 ...\n",
      "Done.\n",
      "Step 2 accuracy: 0.981947700092\n",
      "-----------------------------------\n",
      "\n",
      "Training step: 3 ...\n",
      "Done.\n",
      "Step 3 accuracy: 0.982726496423\n",
      "-----------------------------------\n",
      "\n",
      "Training step: 4 ...\n",
      "Done.\n",
      "Step 4 accuracy: 0.981501424507\n",
      "-----------------------------------\n",
      "\n",
      "Training step: 5 ...\n",
      "Done.\n",
      "Step 5 accuracy: 0.981562678376\n",
      "-----------------------------------\n",
      "\n",
      "Training step: 6 ...\n",
      "Done.\n",
      "Step 6 accuracy: 0.982280219482\n",
      "-----------------------------------\n",
      "\n",
      "Training step: 7 ...\n",
      "Done.\n",
      "Step 7 accuracy: 0.982096458585\n",
      "-----------------------------------\n",
      "\n",
      "Training step: 8 ...\n",
      "Done.\n",
      "Step 8 accuracy: 0.981396418083\n",
      "-----------------------------------\n",
      "\n",
      "Training step: 9 ...\n",
      "Done.\n",
      "Step 9 accuracy: 0.981720186765\n",
      "-----------------------------------\n",
      "\n",
      "10-Fold Cross-Validation accuracy is: 0.981842694144\n"
     ]
    }
   ],
   "source": [
    "cv_results = []\n",
    "for i in range (len(validation_test)):\n",
    "    test_data = validation_test[i]\n",
    "    train_data = validation_training[i]\n",
    "    \n",
    "    print ('Training step:', i, '...' )    \n",
    "    \n",
    "    labels_train, features_train = zip (*train_data)\n",
    "    labels_test, features_test = zip (*test_data)\n",
    "\n",
    "    X_train = np.array(features_train, dtype='float32')\n",
    "    X_test = np.array(features_test, dtype='float32')\n",
    "\n",
    "    y_train = np.array(labels_train)\n",
    "    y_test = np.array(labels_test)    \n",
    "    \n",
    "    stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=0, mode='auto')\n",
    "    batch_size = 100\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape = X_train[0].shape))\n",
    "    model.add(Dense(40))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=200, callbacks= [stop], shuffle=True,  verbose=0,\n",
    "              validation_data=(X_test, y_test))\n",
    "    \n",
    "    print ('Done.')\n",
    "\n",
    "    _, acc = model.evaluate(X_test, y_test, verbose=0, batch_size=batch_size)\n",
    "\n",
    "    cv_results.append(acc)\n",
    "    print ('Step', i, 'accuracy:', acc)\n",
    "    print ('-----------------------------------\\n')\n",
    "    del model\n",
    "\n",
    "cross_val = np.mean(cv_results)\n",
    "print ('10-Fold Cross-Validation accuracy is:', cross_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.shuffle(stops_data)\n",
    "labels, features = zip (*stops_data)\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.01, random_state=42)\n",
    "X_train = np.array(data_train, dtype='float32')\n",
    "X_test = np.array(data_test, dtype='float32')\n",
    "\n",
    "y_train = np.array(labels_train)\n",
    "y_test = np.array(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape = X_train[0].shape))\n",
    "model.add(Dense(40))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=0, mode='auto')\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1131364 samples, validate on 11428 samples\n",
      "Epoch 1/200\n",
      "1131364/1131364 [==============================] - 56s - loss: 0.0584 - acc: 0.9772 - val_loss: 0.0470 - val_acc: 0.9810\n",
      "Epoch 2/200\n",
      "1131364/1131364 [==============================] - 51s - loss: 0.0448 - acc: 0.9817 - val_loss: 0.0468 - val_acc: 0.9808\n",
      "Epoch 3/200\n",
      "1131364/1131364 [==============================] - 60s - loss: 0.0413 - acc: 0.9830 - val_loss: 0.0441 - val_acc: 0.9833\n",
      "Epoch 4/200\n",
      "1131364/1131364 [==============================] - 60s - loss: 0.0389 - acc: 0.9840 - val_loss: 0.0444 - val_acc: 0.9830\n",
      "Epoch 5/200\n",
      "1131364/1131364 [==============================] - 60s - loss: 0.0372 - acc: 0.9847 - val_loss: 0.0435 - val_acc: 0.9837\n",
      "Epoch 6/200\n",
      "1131364/1131364 [==============================] - 60s - loss: 0.0358 - acc: 0.9852 - val_loss: 0.0449 - val_acc: 0.9828\n",
      "Epoch 7/200\n",
      "1131364/1131364 [==============================] - 59s - loss: 0.0346 - acc: 0.9857 - val_loss: 0.0457 - val_acc: 0.9804\n",
      "Epoch 8/200\n",
      "1131364/1131364 [==============================] - 61s - loss: 0.0335 - acc: 0.9862 - val_loss: 0.0465 - val_acc: 0.9828\n",
      "Epoch 9/200\n",
      "1131364/1131364 [==============================] - 61s - loss: 0.0326 - acc: 0.9866 - val_loss: 0.0473 - val_acc: 0.9823\n",
      "11428/11428 [==============================] - 0s     \n",
      "\n",
      "\n",
      "Validation score : 0.0473378441138\n",
      "Validation accuracy : 0.982324126282\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=200, callbacks= [stop], shuffle=True,\n",
    "          validation_data=(X_test, y_test))\n",
    "\n",
    "score, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print ('\\n')\n",
    "print('Validation score :', score)\n",
    "print('Validation accuracy :', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_string = model.to_json()\n",
    "name_ = './Models/Keras_boundary_nn_model_r7_l40_l10_l1'\n",
    "model_name = name_ + '.json'\n",
    "open (model_name, 'w').write(json_string)\n",
    "weights_name = name_ + '_weights.h5'\n",
    "model.save_weights(weights_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on the Opencorpora data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = pd.read_csv('opencorpora.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 : Length of the data set is 0\n",
      "Iteration 10000 : Length of the data set is 10148\n",
      "Iteration 20000 : Length of the data set is 20753\n",
      "Iteration 30000 : Length of the data set is 31276\n",
      "Iteration 40000 : Length of the data set is 42022\n",
      "Iteration 50000 : Length of the data set is 52769\n",
      "Iteration 60000 : Length of the data set is 63280\n",
      "Iteration 70000 : Length of the data set is 74780\n",
      "Iteration 80000 : Length of the data set is 85107\n",
      "Iteration 90000 : Length of the data set is 95527\n"
     ]
    }
   ],
   "source": [
    "first_sentences = list(corpus['sentence'])\n",
    "stops_opencorp_data = collections.deque([])\n",
    "pointer = 0\n",
    "radius = 7\n",
    "window_size = 2*radius+1\n",
    "sliding_window = collections.deque([], maxlen = window_size)\n",
    "dot_features = []\n",
    "\n",
    "for i in xrange(len(first_sentences)-1):\n",
    "    \n",
    "    initial_pointer = 0    \n",
    "    sentence = [' '] + list(unicode (first_sentences[i], 'utf8'))    \n",
    "    \n",
    "    if len(sliding_window) < window_size:\n",
    "        for charnum in range(len(sentence)):\n",
    "            if (charnum == len(sentence) - 1) & stops(sentence[charnum]):\n",
    "                sliding_window.append(sentence[charnum] + u'#')\n",
    "            else:\n",
    "                sliding_window.append(sentence[charnum])\n",
    "            pointer += 1\n",
    "            initial_pointer += 1\n",
    "            if pointer == window_size:\n",
    "                break\n",
    "    \n",
    "    if pointer < window_size:\n",
    "        continue\n",
    "    \n",
    "    for charnum in range (initial_pointer, len(sentence)):\n",
    "        if stops(sliding_window[radius][0]):\n",
    "            dot_features = list(sliding_window)[:radius] + list(sliding_window)[-radius:]\n",
    "            if (len (sliding_window[radius]) == 2):\n",
    "                label = 0\n",
    "            else:\n",
    "                label = 1\n",
    "            vec_features = map (lambda x: embeddings_lookup[x[0]], dot_features)                            \n",
    "            stops_opencorp_data.append((label, vec_features))\n",
    "        if (charnum == len(sentence) - 1) & stops(sentence[charnum]):\n",
    "            sliding_window.append(sentence[charnum] + u'#')\n",
    "        else:\n",
    "            sliding_window.append(sentence[charnum])    \n",
    "    if i % 10000 == 0:\n",
    "        print('Iteration %d : Length of the data set is %d' % (i, len(stops_opencorp_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i in range (len(stops_opencorp_data)):\n",
    "    if stops_opencorp_data[i][0] == 1:\n",
    "        counter +=1\n",
    "print (counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_op, features_op = zip (*stops_opencorp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_op = np.array(features_op, dtype='float32')\n",
    "\n",
    "y_test_op = np.array(labels_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98969/98969 [==============================] - 1s     \n",
      "\n",
      "\n",
      "Test score : 0.129229684986\n",
      "Test accuracy : 0.961725388473\n"
     ]
    }
   ],
   "source": [
    "score_op, acc_op = model.evaluate(X_test_op, y_test_op, batch_size=1000)\n",
    "print ('\\n')\n",
    "print('Test score :', score_op)\n",
    "print('Test accuracy :', acc_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = model_from_json(open('/home/mithfin/anaconda2/docs/Wikiproject/Models/Keras_boundary_nn_model_r7_l40_l10_l1.json').read())\n",
    "model.load_weights('/home/mithfin/anaconda2/docs/Wikiproject/Models/Keras_boundary_nn_model_r7_l40_l10_l1_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
